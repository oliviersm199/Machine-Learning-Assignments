{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 \n",
    "### COMP 551 \n",
    "### Olivier Simard-Morissette \n",
    "### 260563480 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot\n",
    "import seaborn as sns\n",
    "import math\n",
    "sns.set_palette(sns.color_palette(\"GnBu_d\"))\n",
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_mean_from_file(file_path):\n",
    "    '''\n",
    "    Opens the file provided in the instructor to retrieve the means\n",
    "    for the different distributions.\n",
    "    '''\n",
    "    with open(file_path) as file_handler:\n",
    "        # strips out last line because of extra comma in file.\n",
    "        list_of_means = file_handler.read().strip().split(',')[:-1] \n",
    "        list_of_means_float = [np.float(mean) for mean in list_of_means]\n",
    "        array_of_means = np.array(list_of_means_float)\n",
    "        return array_of_means\n",
    " \n",
    "def get_covariance_matrix_from_file(file_path):\n",
    "    # strips out last column because of extra commas. \n",
    "    df_covariance = pd.read_csv(file_path, header=None)\n",
    "    del df_covariance[20]\n",
    "    return df_covariance.as_matrix()\n",
    "\n",
    "\n",
    "\n",
    "m0 = get_mean_from_file('./hwk2_datasets_corrected/DS1_m_0.txt')\n",
    "m1 = get_mean_from_file('./hwk2_datasets_corrected/DS1_m_1.txt')\n",
    "covariance_matrix = get_covariance_matrix_from_file('./hwk2_datasets_corrected/DS1_Cov.txt')\n",
    "\n",
    "m0_dataset = np.random.multivariate_normal(m0, covariance_matrix, size=2000)\n",
    "m1_dataset = np.random.multivariate_normal(m1, covariance_matrix, size=2000)\n",
    "\n",
    "# Labeling the data on the 20th column with 0 corresponding to \n",
    "# a negative example and 1 corresponding to a positive example.\n",
    "df_m0 = pd.DataFrame(m0_dataset)\n",
    "df_m0[20] = 0\n",
    "\n",
    "df_m1 = pd.DataFrame(m1_dataset)\n",
    "df_m1[20] = 1\n",
    "\n",
    "# Concatenating the two datasets into a single dataframe.\n",
    "\n",
    "# Randomizing the datasets \n",
    "df = pd.concat([df_m1, df_m0],ignore_index=True)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "test_df = df[:1200].reset_index(drop=True)\n",
    "train_df = df[1200:].reset_index(drop=True)\n",
    "\n",
    "df.to_csv('./DS1.csv')\n",
    "test_df.to_csv('./DS1_test.csv')\n",
    "train_df.to_csv('./DS1_train.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def _get_column_vector_means(df, class_num):\n",
    "    train_matrix_means = [] \n",
    "    class_df = df[df[20] == class_num]\n",
    "    for column in class_df:\n",
    "        if column != 20:\n",
    "            train_matrix_means.append(class_df[column].mean())\n",
    "    return np.asarray(train_matrix_means)\n",
    "\n",
    "def _get_covariance_sum(df, class_num, mean_vector):\n",
    "    count = 0 \n",
    "    class_df = df[df[20] == class_num] # Only get the sum for this class. \n",
    "    class_df = class_df.drop([20], axis = 1) # Remove the class label \n",
    "    class_df_as_matrix = class_df.as_matrix()\n",
    "    \n",
    "    # Execute summation \n",
    "    covariance_running_sum = np.zeros((20,20))\n",
    "    for i in range(len(class_df_as_matrix)):\n",
    "        mean_difference = class_df_as_matrix[i] - mean_vector\n",
    "        mean_diff_vector = mean_difference.reshape(len(mean_difference),1)\n",
    "        mean_diff_vector_transpose = mean_diff_vector.transpose()\n",
    "        covariance_component = np.divide(np.matmul(mean_diff_vector,mean_diff_vector_transpose),len(df))\n",
    "        covariance_running_sum = np.add(covariance_running_sum, covariance_component )\n",
    "\n",
    "    return covariance_running_sum\n",
    "    \n",
    "def _get_covariance(df, class_nums, class_mean_vectors):\n",
    "    covariance = np.zeros((20,20))\n",
    "    for class_num in class_nums:\n",
    "        mean_vector = class_mean_vectors[class_num]\n",
    "        covariance_sum = _get_covariance_sum(df, class_num, mean_vector)\n",
    "        covariance = np.add(covariance, covariance_sum)\n",
    "    return covariance\n",
    "\n",
    "def _get_class_mean_vectors(df, class_nums):\n",
    "    class_mean_vectors = {}\n",
    "    for class_num in class_nums:\n",
    "        class_mean_vector = _get_column_vector_means(df, class_num)\n",
    "        class_mean_vectors[class_num] = class_mean_vector\n",
    "    return class_mean_vectors \n",
    "\n",
    "def _get_class_probability(df, class_nums):\n",
    "    class_probability = {}\n",
    "    for class_num in class_nums:\n",
    "        class_probability[class_num] = len(df[df[20] == class_num]) / float(len(df))\n",
    "    return class_probability\n",
    "\n",
    "\n",
    "def _decision_boundary(x,class_probability_lookup=None, class_mean_vector_lookup=None, covariance=None):\n",
    "    '''\n",
    "    Takes a test sample x as input and returns true if belonging to class zero,\n",
    "    otherwise returns false \n",
    "    '''\n",
    "    prob_zero = class_probability_lookup[0]\n",
    "    prob_one = class_probability_lookup[1]\n",
    "    covariance_inverse = np.linalg.inv(covariance)\n",
    "    mv_zero = class_mean_vector_lookup[0]\n",
    "    mv_zero = mv_zero.reshape(len(mv_zero), 1)\n",
    "    mv_one = class_mean_vector_lookup[1]\n",
    "    mv_one = mv_one.reshape(len(mv_one), 1)\n",
    "    mv_zero_transpose = mv_zero.transpose()\n",
    "    mv_one_transpose = mv_one.transpose()\n",
    "    x_transpose = x.transpose()\n",
    "    \n",
    "    xtw1 = np.matmul(np.matmul(x_transpose,covariance_inverse),np.subtract(mv_zero,mv_one))\n",
    "    x0_term1 = math.log(prob_zero) - math.log(prob_one)\n",
    "    x0_term2 = -0.5 * np.matmul(np.matmul(mv_zero_transpose,covariance_inverse),mv_zero)\n",
    "    x0_term3 = 0.5 * np.matmul(np.matmul(mv_one_transpose, covariance_inverse),mv_one)\n",
    "    sum_decision = (x0_term1 + x0_term2[0][0] + x0_term3[0][0] +  xtw1[0][0])\n",
    "    return 0 if sum_decision > 0  else 1 \n",
    "\n",
    "\n",
    "def get_classifier(df, class_nums):\n",
    "    '''\n",
    "    Pass \n",
    "    '''\n",
    "    class_mean_vectors = _get_class_mean_vectors(df, class_nums)\n",
    "    covariance = _get_covariance(df, class_nums,class_mean_vectors)\n",
    "    class_probabilities = _get_class_probability(df, class_nums)\n",
    "    return functools.partial(_decision_boundary,\n",
    "                             class_probability_lookup=class_probabilities,\n",
    "                             covariance = covariance,\n",
    "                             class_mean_vector_lookup = class_mean_vectors), covariance, class_mean_vectors\n",
    "\n",
    "\n",
    "def get_measurement_indicators(results):\n",
    "    true_positives = sum([1 for result, answer in results if result == answer and answer == 1])\n",
    "    true_negatives = sum([1 for result, answer in results if result == answer and answer == 0])\n",
    "    false_positives = sum([1 for result, answer in results if result != answer and answer == 0])\n",
    "    false_negatives = sum([1 for result, answer in results if result != answer and answer == 1])\n",
    "\n",
    "    accuracy = float(true_positives + true_negatives) / float(true_positives + false_positives + false_negatives + true_negatives)\n",
    "    precision = float(true_positives) / float(true_positives + false_positives)\n",
    "    recall = float(true_positives) / float(true_positives + false_negatives)\n",
    "    f1_measure = (2 * precision * recall) / (precision + recall)\n",
    "    return accuracy, precision, recall, f1_measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running experiment here \n",
    "class_nums = (0, 1)\n",
    "classifier,covariance,class_mean_vectors = get_classifier(train_df, class_nums)\n",
    "\n",
    "# Reporting the coefficients learnt here for LDA. \n",
    "covariance.tofile('DS1-covariance-matrix')\n",
    "for vector in class_mean_vectors:\n",
    "    vector_arr = class_mean_vectors[vector]\n",
    "    vector_arr.tofile('DS1-Class-%s-Mean-Vector'%(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the results on the test set \n",
    "\n",
    "test_df_vector = test_df.iloc[:,0:20].as_matrix()\n",
    "answers = test_df.iloc[:,20].as_matrix()\n",
    "\n",
    "results = [] \n",
    "for i , row in enumerate(test_df_vector):\n",
    "    answer = answers[i]\n",
    "    row = row.reshape(len(row),1)\n",
    "    result = classifier(row)\n",
    "    results.append((result,answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following definitions were used for true positive and true negative. They were taken from \n",
    "the slide 15 and 17 from the lecture on model evaluation.\n",
    "<br />\n",
    "True positive: Example of class 1 predicted as class 1.\n",
    "<br />\n",
    "False positive: Example of class 0 predicted as class 1. \n",
    "<br />\n",
    "True negative: Example of class 0 predicted as class 0.\n",
    "<br />\n",
    "False negative: Example of class 1 predicted as class 0. \n",
    "<br/>\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
    "<br /> \n",
    "Precision = True positives / Total number of declared positives = TP / (TP+ FP)\n",
    "<br />\n",
    "Recall = True positives / Total number of actual positives = TP / (TP + FN)\n",
    "<br />\n",
    "F1 measure = (2 * ( Precision * Recall ) ) / ( precision + recall )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9625\n",
      "Precision:0.9646\n",
      "Recall:0.9630\n",
      "F1 Measure:0.9638\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1_measure = get_measurement_indicators(results)\n",
    "\n",
    "print(\"Accuracy:%.4f\"%(accuracy))\n",
    "print(\"Precision:%.4f\"%(precision))\n",
    "print(\"Recall:%.4f\"%(recall))\n",
    "print(\"F1 Measure:%.4f\"%(f1_measure))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "The algorithm for k-nearest neighbours used in this assignment is taken from slide 28, lecture 7 on Instance Learning. During training, the data points are just stored. When making a prediction, the euclidean distance is used as our distance measure to compare the input vector with all training data points. Then I sort to get the vectors which had the smallest euclidean distance from the input vector and classify by looking at the majority class of k vectors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def _get_knn_classifier(x,train_data=None, class_labels = None, k=None):\n",
    "    sample_norms = np.linalg.norm(x - train_data,axis=1)\n",
    "    sample_scores_with_cl = [(score,label) for score,label in zip(sample_norms, class_labels)]\n",
    "    sample_scores_cl_sorted = sorted(sample_scores_with_cl, key = lambda x : x[0])\n",
    "    sample_scores_k_group = [label for score, label in sample_scores_cl_sorted[:k]]\n",
    "    c = Counter(sample_scores_k_group)\n",
    "    return c.most_common()[0][0] # get the most common\n",
    "    \n",
    "    \n",
    "    \n",
    "def knn_classifier(train_data, class_labels,k):\n",
    "    return functools.partial(_get_knn_classifier,\n",
    "                             train_data=train_data, \n",
    "                             class_labels = class_labels,\n",
    "                             k=k)\n",
    "\n",
    "\n",
    "    \n",
    "train_df_matrix = train_df.as_matrix()\n",
    "\n",
    "train_class_labels = train_df_matrix[:,20]\n",
    "train_data = train_df_matrix[:,0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Accuracy:0.5183\n",
      "Precision:0.5109\n",
      "Recall:0.5541\n",
      "F1 Measure:0.5316\n",
      "2\n",
      "Accuracy:0.5000\n",
      "Precision:0.4884\n",
      "Recall:0.2855\n",
      "F1 Measure:0.3603\n",
      "3\n",
      "Accuracy:0.5300\n",
      "Precision:0.5216\n",
      "Recall:0.5709\n",
      "F1 Measure:0.5452\n",
      "4\n",
      "Accuracy:0.5425\n",
      "Precision:0.5497\n",
      "Recall:0.4020\n",
      "F1 Measure:0.4644\n",
      "5\n",
      "Accuracy:0.5608\n",
      "Precision:0.5509\n",
      "Recall:0.5946\n",
      "F1 Measure:0.5719\n",
      "6\n",
      "Accuracy:0.5442\n",
      "Precision:0.5460\n",
      "Recall:0.4510\n",
      "F1 Measure:0.4940\n",
      "7\n",
      "Accuracy:0.5675\n",
      "Precision:0.5542\n",
      "Recall:0.6301\n",
      "F1 Measure:0.5897\n",
      "8\n",
      "Accuracy:0.5567\n",
      "Precision:0.5573\n",
      "Recall:0.4932\n",
      "F1 Measure:0.5233\n",
      "9\n",
      "Accuracy:0.5642\n",
      "Precision:0.5511\n",
      "Recall:0.6284\n",
      "F1 Measure:0.5872\n",
      "10\n",
      "Accuracy:0.5558\n",
      "Precision:0.5539\n",
      "Recall:0.5118\n",
      "F1 Measure:0.5320\n",
      "11\n",
      "Accuracy:0.5558\n",
      "Precision:0.5429\n",
      "Recall:0.6301\n",
      "F1 Measure:0.5833\n",
      "12\n",
      "Accuracy:0.5500\n",
      "Precision:0.5450\n",
      "Recall:0.5321\n",
      "F1 Measure:0.5385\n",
      "13\n",
      "Accuracy:0.5508\n",
      "Precision:0.5387\n",
      "Recall:0.6233\n",
      "F1 Measure:0.5779\n",
      "14\n",
      "Accuracy:0.5458\n",
      "Precision:0.5410\n",
      "Recall:0.5236\n",
      "F1 Measure:0.5322\n",
      "15\n",
      "Accuracy:0.5475\n",
      "Precision:0.5363\n",
      "Recall:0.6115\n",
      "F1 Measure:0.5714\n",
      "16\n",
      "Accuracy:0.5483\n",
      "Precision:0.5427\n",
      "Recall:0.5372\n",
      "F1 Measure:0.5399\n",
      "17\n",
      "Accuracy:0.5542\n",
      "Precision:0.5414\n",
      "Recall:0.6301\n",
      "F1 Measure:0.5824\n",
      "18\n",
      "Accuracy:0.5575\n",
      "Precision:0.5514\n",
      "Recall:0.5524\n",
      "F1 Measure:0.5519\n",
      "19\n",
      "Accuracy:0.5667\n",
      "Precision:0.5531\n",
      "Recall:0.6334\n",
      "F1 Measure:0.5906\n",
      "20\n",
      "Accuracy:0.5717\n",
      "Precision:0.5648\n",
      "Recall:0.5743\n",
      "F1 Measure:0.5695\n"
     ]
    }
   ],
   "source": [
    "test_vectors = test_df.as_matrix()[:,0:20]\n",
    "test_class_labels = test_df.as_matrix()[:,20]\n",
    "\n",
    "measurements = [] \n",
    "for k in range(1,21):\n",
    "    knn_clfr = knn_classifier(train_data, train_class_labels, k)\n",
    "    \n",
    "    results = []\n",
    "    for test_vector, class_label in zip(test_vectors, test_class_labels):\n",
    "        prediction = knn_clfr(test_vector)\n",
    "        results.append((prediction, class_label))\n",
    "\n",
    "\n",
    "    accuracy, precision, recall, f1_measure = get_measurement_indicators(results)\n",
    "    \n",
    "    print(k)\n",
    "    print(\"Accuracy:%.4f\"%(accuracy))\n",
    "    print(\"Precision:%.4f\"%(precision))\n",
    "    print(\"Recall:%.4f\"%(recall))\n",
    "    print(\"F1 Measure:%.4f\"%(f1_measure))\n",
    "    \n",
    "    measurements.append((k,accuracy,precision,recall,f1_measure))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Do you do better or worse than LDA?*\n",
    "\n",
    "The performance of KNN across all evaluation measurements was worse than linear discriminant analysis. \n",
    "\n",
    "*Are there particular values of k which perform better?* \n",
    "\n",
    "If a value of k is even, it has a much lower recall than a value of k which is odd. \n",
    "\n",
    "*Report the best fit accuracy, precision, recall and f-measure achieved by this classifier.*\n",
    "\n",
    "The best fit was determined by looking at the Accuracy from K = 1 to 20. This was determined to be when k=13. The accuracy, precision, recall  and F1 were:\n",
    "\n",
    "k = 13\n",
    "<br />\n",
    "Accuracy:0.5508\n",
    "<br />\n",
    "Precision:0.5387\n",
    "<br />\n",
    "Recall:0.6233\n",
    "<br />\n",
    "F1 Measure:0.5779\n",
    "<br />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import choice\n",
    "\n",
    "c1_m1_mean = get_mean_from_file('./hwk2_datasets_corrected/DS2_c1_m1.txt')\n",
    "c1_m2_mean = get_mean_from_file('./hwk2_datasets_corrected/DS2_c1_m2.txt')\n",
    "c1_m3_mean = get_mean_from_file('./hwk2_datasets_corrected/DS2_c1_m3.txt')\n",
    "c2_m1_mean = get_mean_from_file('./hwk2_datasets_corrected/DS2_c2_m1.txt')\n",
    "c2_m2_mean = get_mean_from_file('./hwk2_datasets_corrected/DS2_c2_m2.txt')\n",
    "c2_m3_mean = get_mean_from_file('./hwk2_datasets_corrected/DS2_c2_m3.txt')\n",
    "covariance_one = get_covariance_matrix_from_file('./hwk2_datasets_corrected/DS2_Cov1.txt')\n",
    "covariance_two = get_covariance_matrix_from_file('./hwk2_datasets_corrected/DS2_Cov2.txt')\n",
    "covariance_three = get_covariance_matrix_from_file('./hwk2_datasets_corrected/DS2_Cov3.txt')\n",
    "\n",
    "class_nums = (0, 1)\n",
    "\n",
    "means = {\n",
    "    (0,\"c1\"):c1_m1_mean,\n",
    "    (0,\"c2\"):c1_m2_mean,\n",
    "    (0,\"c3\"):c1_m3_mean,\n",
    "    (1,\"c1\"):c2_m1_mean,\n",
    "    (1,\"c2\"):c2_m2_mean,\n",
    "    (1,\"c3\"):c2_m3_mean\n",
    "}\n",
    "\n",
    "covariances = {\n",
    "    \"c1\": covariance_one,\n",
    "    \"c2\": covariance_two,\n",
    "    \"c3\": covariance_three\n",
    "}\n",
    "\n",
    "samples = [] \n",
    "for class_num in class_nums:\n",
    "    class_samples = [] \n",
    "    for _ in range(2000):\n",
    "        sample_class = choice([\"c1\",\"c2\",\"c3\"], 1, p=[0.1, 0.42, 0.48])\n",
    "        sample_mean = means[(class_num,sample_class[0])]\n",
    "        sample_covariance = covariances[sample_class[0]]\n",
    "        sample = np.random.multivariate_normal(sample_mean, sample_covariance)\n",
    "        class_samples.append(sample)\n",
    "    samples.append(np.asarray(class_samples))\n",
    "\n",
    "class_one_df = pd.DataFrame(samples[0])\n",
    "class_one_df[20] = 0\n",
    "class_two_df = pd.DataFrame(samples[1])\n",
    "class_two_df[20] = 1\n",
    "\n",
    "\n",
    "# Randomizing the datasets \n",
    "df = pd.concat([class_one_df, class_two_df],ignore_index=True)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "test_df = df[:1200].reset_index(drop=True)\n",
    "train_df = df[1200:].reset_index(drop=True)\n",
    "\n",
    "df.to_csv('./DS2.csv')\n",
    "test_df.to_csv('./DS2_test.csv')\n",
    "train_df.to_csv('./DS2_train.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.5375\n",
      "Precision:0.5352\n",
      "Recall:0.5343\n",
      "F1 Measure:0.5348\n"
     ]
    }
   ],
   "source": [
    "# Running experiment with LDA \n",
    "\n",
    "classifier,covariance,class_mean_vectors = get_classifier(train_df, class_nums)\n",
    "\n",
    "\n",
    "covariance.tofile('DS2-covariance-matrix')\n",
    "for vector in class_mean_vectors:\n",
    "    vector_arr = class_mean_vectors[vector]\n",
    "    vector_arr.tofile('DS2-Class-%s-Mean-Vector'%(vector))\n",
    "    \n",
    "\n",
    "test_df_vector = test_df.iloc[:,0:20].as_matrix()\n",
    "answers = test_df.iloc[:,20].as_matrix()\n",
    "\n",
    "results = [] \n",
    "for i , row in enumerate(test_df_vector):\n",
    "    answer = answers[i]\n",
    "    row = row.reshape(len(row),1)\n",
    "    result = classifier(row)\n",
    "    results.append((result,answer))\n",
    "  \n",
    "\n",
    "accuracy, precision, recall, f1_measure = get_measurement_indicators(results)\n",
    "\n",
    "print(\"Accuracy:%.4f\"%(accuracy))\n",
    "print(\"Precision:%.4f\"%(precision))\n",
    "print(\"Recall:%.4f\"%(recall))\n",
    "print(\"F1 Measure:%.4f\"%(f1_measure))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Accuracy:0.5325\n",
      "Precision:0.5306\n",
      "Recall:0.5226\n",
      "F1 Measure:0.5266\n",
      "2\n",
      "Accuracy:0.5258\n",
      "Precision:0.5500\n",
      "Recall:0.2580\n",
      "F1 Measure:0.3512\n",
      "3\n",
      "Accuracy:0.5167\n",
      "Precision:0.5150\n",
      "Recall:0.4874\n",
      "F1 Measure:0.5009\n",
      "4\n",
      "Accuracy:0.5383\n",
      "Precision:0.5609\n",
      "Recall:0.3317\n",
      "F1 Measure:0.4168\n",
      "5\n",
      "Accuracy:0.5250\n",
      "Precision:0.5241\n",
      "Recall:0.4925\n",
      "F1 Measure:0.5078\n",
      "6\n",
      "Accuracy:0.5275\n",
      "Precision:0.5385\n",
      "Recall:0.3518\n",
      "F1 Measure:0.4255\n",
      "7\n",
      "Accuracy:0.5283\n",
      "Precision:0.5275\n",
      "Recall:0.4975\n",
      "F1 Measure:0.5121\n",
      "8\n",
      "Accuracy:0.5225\n",
      "Precision:0.5300\n",
      "Recall:0.3551\n",
      "F1 Measure:0.4253\n",
      "9\n",
      "Accuracy:0.5283\n",
      "Precision:0.5275\n",
      "Recall:0.4975\n",
      "F1 Measure:0.5121\n",
      "10\n",
      "Accuracy:0.5383\n",
      "Precision:0.5531\n",
      "Recall:0.3752\n",
      "F1 Measure:0.4471\n",
      "11\n",
      "Accuracy:0.5433\n",
      "Precision:0.5451\n",
      "Recall:0.4958\n",
      "F1 Measure:0.5193\n",
      "12\n",
      "Accuracy:0.5375\n",
      "Precision:0.5510\n",
      "Recall:0.3802\n",
      "F1 Measure:0.4500\n",
      "13\n",
      "Accuracy:0.5517\n",
      "Precision:0.5526\n",
      "Recall:0.5193\n",
      "F1 Measure:0.5354\n",
      "14\n",
      "Accuracy:0.5550\n",
      "Precision:0.5708\n",
      "Recall:0.4255\n",
      "F1 Measure:0.4875\n",
      "15\n",
      "Accuracy:0.5592\n",
      "Precision:0.5612\n",
      "Recall:0.5226\n",
      "F1 Measure:0.5412\n",
      "16\n",
      "Accuracy:0.5592\n",
      "Precision:0.5752\n",
      "Recall:0.4355\n",
      "F1 Measure:0.4957\n",
      "17\n",
      "Accuracy:0.5608\n",
      "Precision:0.5639\n",
      "Recall:0.5176\n",
      "F1 Measure:0.5397\n",
      "18\n",
      "Accuracy:0.5492\n",
      "Precision:0.5639\n",
      "Recall:0.4137\n",
      "F1 Measure:0.4773\n",
      "19\n",
      "Accuracy:0.5483\n",
      "Precision:0.5514\n",
      "Recall:0.4941\n",
      "F1 Measure:0.5212\n",
      "20\n",
      "Accuracy:0.5475\n",
      "Precision:0.5611\n",
      "Recall:0.4154\n",
      "F1 Measure:0.4774\n"
     ]
    }
   ],
   "source": [
    "# Running experiment with KNN\n",
    "\n",
    "train_df_matrix = train_df.as_matrix()\n",
    "train_class_labels = train_df_matrix[:,20]\n",
    "train_data = train_df_matrix[:,0:20]\n",
    "\n",
    "test_df_vector = test_df.iloc[:,0:20].as_matrix()\n",
    "answers = test_df.iloc[:,20].as_matrix()\n",
    "\n",
    "for k in range(1,21):\n",
    "    knn_clfr = knn_classifier(train_data, train_class_labels, k)\n",
    "    \n",
    "    results = []\n",
    "    for test_vector, class_label in zip(test_df_vector, answers):\n",
    "        prediction = knn_clfr(test_vector)\n",
    "        results.append((prediction, class_label))\n",
    "\n",
    "\n",
    "    accuracy, precision, recall, f1_measure = get_measurement_indicators(results)\n",
    "    \n",
    "    print(k)\n",
    "    print(\"Accuracy:%.4f\"%(accuracy))\n",
    "    print(\"Precision:%.4f\"%(precision))\n",
    "    print(\"Recall:%.4f\"%(recall))\n",
    "    print(\"F1 Measure:%.4f\"%(f1_measure))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best fit was determined by determing the highest accuracy from K = 1 to 20. This was determined to be when k=15. The accuracy, precision, recall and F1 were:\n",
    "\n",
    "k = 15 \n",
    "<br />\n",
    "Accuracy:0.5592\n",
    "<br />\n",
    "Precision:0.5612\n",
    "<br />\n",
    "Recall:0.5226\n",
    "<br />\n",
    "F1 Measure:0.5412\n",
    "\n",
    "The performance of the LDA is reduced dramatically. This can be explained by LDA's assumption that all sample classes have the same covariance matrices. In this case, the samples were generated using separate covariance matrices and we got a poor performance as a result. \n",
    "\n",
    "The performance of the KNN did not improve from DS1 to DS2. The performance of KNN in this case was better than LDA when an odd number of K was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
